{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c835d7",
   "metadata": {},
   "source": [
    "# Consensus\n",
    "Consensus is support for:  \n",
    "- Classification (Label IoU)\n",
    "- Box (IoU)\n",
    "- Polygon (IoU)\n",
    "- Semantic Segmentation (IoU)\n",
    "- Point (distance scoring)\n",
    "\n",
    "## IoU\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://storage.googleapis.com/kaggle-media/competitions/rsna/IoU.jpg\" width=\"350\" title=\"IoU\">\n",
    "</p>\n",
    "\n",
    "Let's begin with some necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a4c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtlpy as dl\n",
    "from dtlpy.ml import metrics, predictions_utils\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee05142",
   "metadata": {},
   "source": [
    "## Classification Consensus and Majority Vote\n",
    "\n",
    "To get the item IoU score, we first calculate each annotator's annotation set against all the other annotation sets. Here, we'll have 5 annotators annotating the same item for example. \n",
    "\n",
    "But first we'll use the dtlpy SDK to get all items and all annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8103e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-17 11:49:59][ERR][dtlpy:v1.95.6][services.api_client:1398] [Response <404>][Reason: Not Found][Text: {\"status\":404,\"message\":\"item not found while resolving dataset\"}]\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "('404', 'item not found while resolving dataset')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m first_item \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m6215d3f73750a54742c4d33d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m second_item \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mget(item_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6215d3fee2b78c63e4ae501f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m third_item \u001b[38;5;241m=\u001b[39m dl\u001b[38;5;241m.\u001b[39mitems\u001b[38;5;241m.\u001b[39mget(item_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6215d4053750a5fe47c4d343\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\Python3.8\\lib\\site-packages\\dtlpy\\repositories\\items.py:268\u001b[0m, in \u001b[0;36mItems.get\u001b[1;34m(self, filepath, item_id, fetch, is_dir)\u001b[0m\n\u001b[0;32m    262\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch found in items.get: filepath is different then item.filename: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    265\u001b[0m                     filepath,\n\u001b[0;32m    266\u001b[0m                     item\u001b[38;5;241m.\u001b[39mfilename))\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexceptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPlatformException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     filters \u001b[38;5;241m=\u001b[39m entities\u001b[38;5;241m.\u001b[39mFilters()\n",
      "File \u001b[1;32mC:\\Python\\Python3.8\\lib\\site-packages\\dtlpy\\exceptions.py:49\u001b[0m, in \u001b[0;36mPlatformException.__init__\u001b[1;34m(self, error, message)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m exceptions:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code](status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnknownException(status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mNotFound\u001b[0m: ('404', 'item not found while resolving dataset')"
     ]
    }
   ],
   "source": [
    "first_item = dl.items.get(item_id='6215d3f73750a54742c4d33d')\n",
    "second_item = dl.items.get(item_id='6215d3fee2b78c63e4ae501f')\n",
    "third_item = dl.items.get(item_id='6215d4053750a5fe47c4d343')\n",
    "fourth_item = dl.items.get(item_id='6215d40ce2b78c3b85ae5022')\n",
    "fifth_item = dl.items.get(item_id='6215d415e2b78c36b7ae5028')\n",
    "\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()\n",
    "third_annotations = third_item.annotations.list()\n",
    "fourth_annotations = fourth_item.annotations.list()\n",
    "fifth_annotations = fifth_item.annotations.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472670f",
   "metadata": {},
   "source": [
    "Let's see which labels each annotator tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85f6b026",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_item' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_item\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with list of annotations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[annotation\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mannotation\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfirst_annotations]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecond_item\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with list of annotations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[annotation\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mannotation\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39msecond_annotations]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthird_item\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with list of annotations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[annotation\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mannotation\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mthird_annotations]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'first_item' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'{first_item.name} with list of annotations: {[annotation.label for annotation in first_annotations]}')\n",
    "print(f'{second_item.name} with list of annotations: {[annotation.label for annotation in second_annotations]}')\n",
    "print(f'{third_item.name} with list of annotations: {[annotation.label for annotation in third_annotations]}')\n",
    "print(f'{fourth_item.name} with list of annotations: {[annotation.label for annotation in fourth_annotations]}')\n",
    "print(f'{fifth_item.name} with list of annotations: {[annotation.label for annotation in fifth_annotations]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaced85",
   "metadata": {},
   "source": [
    "So in order to create the annotators' scoring, we'll go over all and to the IoU calculation and save it in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_list = [first_item, second_item, third_item, fourth_item, fifth_item]\n",
    "n_annotators = len(items_list)\n",
    "items_scores = np.zeros((n_annotators, n_annotators))\n",
    "for i_item in range(n_annotators):\n",
    "    for j_item in range(n_annotators):\n",
    "        # note: the results matrix is symmetric so calculation can be done only on one side of the diagonal\n",
    "        # we do both sides to show that the score is the same: measure_itemx(x, y) == measure_itemx(y, x)\n",
    "        success, results = predictions_utils.measure_item(items_list[i_item], items_list[j_item] ,ignore_labels=False)\n",
    "        items_scores[i_item, j_item] = results['total_mean_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50aba69",
   "metadata": {},
   "source": [
    "The returned Result object contains a pandas DataFrame with all matching and scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9407b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=False)\n",
    "results[dl.AnnotationType.CLASSIFICATION].to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908657bd",
   "metadata": {},
   "source": [
    "We'll use the seaborn package to plot the metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec344d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(items_scores, \n",
    "            annot=True, \n",
    "            cmap='Blues',\n",
    "            xticklabels=['Annotator A','Annotator B','Annotator C', 'Annotator D', 'Annotator E'],\n",
    "            yticklabels=['Annotator A','Annotator B','Annotator C', 'Annotator D', 'Annotator E'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d18d4",
   "metadata": {},
   "source": [
    "Annotator A had ['A', 'B'] and B had ['C', 'B']. The union is ['A', 'B', 'C'], intersection is only ['B'] which should give 33% match as we can see in the metrix.\n",
    "Counting the appearances of each label give the fllowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f985b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count per label\n",
    "from collections import Counter\n",
    "all_annotations = [first_annotations, second_annotations,third_annotations,fourth_annotations,fifth_annotations]\n",
    "all_labels = [annotation.label for annotations in all_annotations for annotation in annotations]\n",
    "counter = Counter(all_labels)\n",
    "for label, count in counter.items():\n",
    "    print('{}: {}'.format(label, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068bd31",
   "metadata": {},
   "source": [
    "And if we want to output all the majority annotations (3 or more annotator gave the same label) we will get \n",
    "['B', 'C'] as the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d1212",
   "metadata": {},
   "source": [
    "## Box IoU Matching\n",
    "\n",
    "Box matching is basiclly the same. We'll get the items and annotations and \"show()\" the annotation of each item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7beccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214bc0d3750a50f50c44841')\n",
    "second_item = dl.items.get(item_id='6214be90fed92a9f043ba217')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('first')\n",
    "plt.figure()\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('second')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c93e2",
   "metadata": {},
   "source": [
    "Now lets overlay the annotation on top of each other so see the matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_annotations.show())\n",
    "plt.imshow(second_annotations.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d14e05",
   "metadata": {},
   "source": [
    "Running the comparison over the two items will give the results Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item,ignore_labels=True)\n",
    "results[dl.AnnotationType.BOX].to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab34fb",
   "metadata": {},
   "source": [
    "We used the \"ignore_labels=True\" flag so the matching ignores the label. This means the yellow and the red anntoations at the top right are a match. If we will run the same functino without the flag we will get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item, ignore_labels=False)\n",
    "results[dl.AnnotationType.BOX].to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92785f",
   "metadata": {},
   "source": [
    "Now we are getting only two matches (two gressn and two blues) and 3 unmatched annotations (one of the blues, red and yellow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a4e6e1",
   "metadata": {},
   "source": [
    "DEBUG: View the annotaqtion comparison matrix for each two items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68484b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['box'].matches._annotations_raw_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea21b23",
   "metadata": {},
   "source": [
    "List of all the matching scores and the mean over the item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ea970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['box'].to_df()['annotation_score'])\n",
    "print(results['total_mean_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae36ae9",
   "metadata": {},
   "source": [
    "## Polygon and Segmentation\n",
    "\n",
    "Save as all the above, IoU scoring for two example images and anntoations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fa630",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214d07599cb175c9cd73d8f')\n",
    "second_item = dl.items.get(item_id='6214d07c9d80b05b8310ba9b')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('first')\n",
    "plt.figure()\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6de757",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, results = predictions_utils.measure_item(first_item, second_item,ignore_labels=True,match_threshold=0)\n",
    "results[dl.AnnotationType.SEGMENTATION].to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[dl.AnnotationType.SEGMENTATION].matches._annotations_raw_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d66437",
   "metadata": {},
   "source": [
    "And the total score for this items is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['total_mean_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcb260",
   "metadata": {},
   "source": [
    "## Three Sets Comparison\n",
    "In order to match across multiple annotators, we are calculating the scoring metrix between all couples of anntoators.\n",
    "In this example we'll see 3 annotators with bounding box annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = dl.items.get(item_id='6214ea1d0ec695cd9c35dfbd')\n",
    "second_item = dl.items.get(item_id='6214ea29e2b78c7ca1adc6b7')\n",
    "third_item = dl.items.get(item_id='6214ea310ec695600635dfc6')\n",
    "first_annotations = first_item.annotations.list()\n",
    "second_annotations = second_item.annotations.list()\n",
    "third_annotations = third_item.annotations.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3112a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(first_annotations.show())\n",
    "plt.title('first')\n",
    "plt.figure()\n",
    "plt.imshow(second_annotations.show())\n",
    "plt.title('second')\n",
    "plt.figure()\n",
    "plt.imshow(third_annotations.show())\n",
    "plt.title('third')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d4fd0",
   "metadata": {},
   "source": [
    "Plotting all three on top of each other with different thickness to differ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(first_annotations.show(thickness=20))\n",
    "plt.imshow(second_annotations.show(thickness=10))\n",
    "plt.imshow(third_annotations.show(thickness=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [first_item, second_item, third_item]\n",
    "n_annotators = len(items)\n",
    "items_scores = np.zeros((n_annotators, n_annotators))\n",
    "for i_item in range(n_annotators):\n",
    "    for j_item in range(i_item, n_annotators):\n",
    "        success, results = predictions_utils.measure_item(items[i_item], items[j_item], ignore_labels=True)\n",
    "        items_scores[i_item, j_item] = results['total_mean_score']      \n",
    "        items_scores[j_item, i_item] = results['total_mean_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(items_scores, \n",
    "            annot=True, \n",
    "            cmap='Blues',\n",
    "            xticklabels=['Annotator A','Annotator B','Annotator C'],\n",
    "            yticklabels=['Annotator A','Annotator B','Annotator C'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
